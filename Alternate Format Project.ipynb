{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier in Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layal Bata and Bryce Boyle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you characterize the following review of the Eagle Rock icon, Walt's Bar?\n",
    "\"Walt's bar is a hole in the wall that looks like something out of an 80s after school special\n",
    "It's got a couple of pinball machine and serves bar food and drinks. Not a wide selection of beers but it's a cute little space.\" -Maria Bazan, Local Guide\n",
    "\n",
    "Is it positive? Negative? What words make you think that? Now, how would a computer interpet this review?\n",
    "This is a common question addressed in Natural Language Processing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP is an area of computer science that focuses on gathering meaning from written or spoken language in a way a computer would understand it. If you've used Siri to call someone, or autocorrect to fix a typo, you've benefitted from NLP!\n",
    "\n",
    "Here, we're going to use NLP techniques to figure out if a restaurant review is positive or negative based on the words used. But we'll get to that later! First, let's discuss sentiment analysis in text classification.\n",
    "\n",
    "In NLP, text classification is when you categorize words based on characteristics of the words. For our purposes, we'll be looking at whether a word has a positive or negative connotation, analyzing the \"sentiment\" of the word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes this can be very difficult to do because computers don't have the context or general knowledge we apply automatically when we're looking at language. One attempted solution to this problem is the Naive Bayes' Classifier, the limitations of which we will definitely get into later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand NLP a little better, let's look at how we can use a Naive Bayes' Classifier to help us understand our restaurant review data we mentioned earlier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna look at some math here, but we promise we'll break it down so stick with us!\n",
    " \n",
    "Bayes' theorem is used in conditional probability. This is the probability that something will happen given something else is true. For example, the probability that a candidate will win an election given they're the incumbent, or the probability that your friend will like limeade given that they like lemonade. \n",
    "\n",
    "Bayes' Theorem is defined as: <br>$$P(A|B) = \\displaystyle\\frac {P(B|A)P(A)} {P(B)}$$\n",
    "\n",
    "Let's say the probability your friend likes limeade is P(A), and the probability your friend likes lemonade is P(B). To calculate the probability that your friend likes limeade given they like lemonade P(B|A), we need 3 probabilities: the probability that your friend likes lemonade given they like limeade P(A|B), the probability your friend likes lemonade P(B), and the probability your friend likes limeade P(A).\n",
    "\n",
    "So according to Bayes' Theorem:\n",
    "probability(limeade|lemonade) = (probability(lemonade|limeade)*probability(lemonade)) / probability(limeade)\n",
    "\n",
    "Let's calculate this together! Let's say that the probability of someone liking lemonade given they like limeade is pretty high, 80% (0.8). Think of one of your friends, and make up the next two probabilites based on your friend's taste!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friendName = input(\"What's your friend's name? \")\n",
    "limeade = input(\"What's the probability that \" +friendName + \" likes limeade as a decimal (0.11-0.99)?\")\n",
    "lemonade = input(\"What's the probability that \" +friendName+ \" likes lemonade as a decimal (0.11-0.99)?\")\n",
    "BGivenA = float(0.8)\n",
    "numerator = BGivenA*float(limeade)\n",
    "AGivenB = round(100*(numerator/float(lemonade)),2)\n",
    "print(\"The probability \" +friendName+ \" likes limeade given they like lemonade is: \" +str(AGivenB)+ \"%!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wasn't that fun! Now that we have a solid understanding of Bayes Theorem, let's find out how to use this for NLP!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through this word by word!\n",
    "\n",
    "We'll start with the end; the \"Classifier\" in \"Naive Bayes Classifier\" tells us how we're going to deal with our data. Classification is the process of predicting the \"class\" of a data point. In the example of our restaurant reviews, our classes are \"positive\" or \"negative.\" We build up what we decide to be positive and negative by splitting our data into training and testing sets. The training set is where our classifier uses the training data to understand how our input data (our reviews) relate to the class (positive or negative). In our classifier, known positive and negative reviews are used as our training data. Once it's done with the training data, we then use that information to classify unknown reviews and decide if they're positive or negative.\n",
    "\n",
    "The \"Bayes\" in \"Naive Bayes Classifier\" means that we're applying Bayes Theorem to each of our reviews. Given the probailities of each word in the review being in a positive review (gathered in our training set), we calculate how likely it is that the review is positive. We then use this probability to classify whether or not the review is positive.\n",
    "\n",
    "The \"Naive\" in \"Naive Bayes Classifier\" means that each piece of data we put into our model is independent of the others. This means that if we're trying to decide if the review \"Loved the atmosphere! Food was great!\" is positive or negative, how we classify \"Loved\" has no effect on how we classify \"atmosphere.\" In creating a Naive Bayes Classifier model, we are making the assumption that the classification of one word doesn't affect the classification of another.\n",
    "\n",
    "Putting it all together, our Naive Bayes Classifier takes each of our restaurant reviews, applies Bayes' Theorem to find the probability of the review being positive or negative given each word in the review, and then classifies that review as positive or negative based on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gone over how the Naive Bayes' Classifier works, let's get into the implementation and have it sort some restaurant reviews! The dataset are using can be found [here](https://www.kaggle.com/vigneshwarsofficial/reviews), and consists of 1000 restaurant reviews that have been labelled as either positive or negative. The first few lines of code below are just for setup and aren't relevant to this walkthrough, but they still need to be run before the other kernels will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import*\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "dataframe = pd.read_csv('restaurantrevievs.csv') #make sure the data file is in the same folder as the notebook! if not, copy file path and paste here\n",
    "review_list = dataframe[' Review'].tolist() # grabbing reviews and putting them into a list (one review per element of list)\n",
    "rating_list = dataframe['Liked'].tolist() # same as above but with the ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in analyzing any kind of data is making sure it's in a form that is easy to work with. In the code above, we extracted the restaurant reviews and ratings from a large file where all of the information was separated by commas. But there is still more to do! In order to apply Bayes' Theorem correctly, we need to organize the data into two main collections: a list of every word in the dataset, and a list of all reviews labelled as positive or negative.\n",
    "\n",
    "To create the list of all words, we have to sort through all of the reviews and add the words that we want. Right now, the reviews contain punctuation and many words without actual meaning, such as \"and\", \"a\", and \"the\" that don't contribute to the sentiment of the review. In NLP, these words are referred to as stop words, and are usually removed from text before we analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopWords(words):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    content = [w for w in words if w.lower() not in stopword_list]\n",
    "    return content\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    content = [w for w in words if w.isalpha()]\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have named this list of all words \"all_words\" (creative, we know!) and  removed punctuation and stop words from it. In NLP, all_words is called a \"bag of words\" because every word is extracted separately without any information about surrounding words. For example, if a review contains the phrase \"not bad\", then both \"not\" and \"bad\" are tossed into the list with no context (you can imagine why there'd be issues with this...). This list will be used later on for training and testing the classifier to help us find the most commonly used words.\n",
    "\n",
    "Now that our bag of words has been created and sorted through, we make a list called \"documents\" that will hold all of the reiviews and information on whether each one is considered positive or negative. We do this by labelling each review with 'pos' or 'neg' depending on how it was originally labelled in the dataset. Then, we shuffle the order of the reviews in our list so that the classifier results are more consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "all_words = []\n",
    "\n",
    "for i in range(len(review_list)):\n",
    "    if rating_list[i] == 1:           # if the rating is positive, add to documents and label as 'pos'\n",
    "        documents.append([i, 'pos'])\n",
    "\n",
    "    elif rating_list[i] == 0:         # if the rating is negative, add to documents and label as 'neg'\n",
    "        documents.append([i, 'neg'])\n",
    "\n",
    "    for word in nltk.word_tokenize(review_list[i]):     # add all individual words in each review to all_words (bag of words)\n",
    "        all_words.append(word)\n",
    "\n",
    "all_words = remove_punctuation(remove_stopWords(all_words)) # removing punctuation and stop words\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the fun part! Our data has been organized into useful lists (all_words and documents), and now we have to go through a few additional steps to ensure that our classifier has all of the information it needs.\n",
    "\n",
    "Below, we convert all_words into an object that contains information about how frequently each word appears in the list. The 2000 most common words found in the list are then added to a new list called \"word_features\". Finally, each review is analyzed for which of the most common words it contains (from word_features) and whether the review as a whole is positive or negative. This will help our classifier determine what words might be included in a negative review and which are more likely to be found in a positive review. This information is then added to another list called feature_sets that will be used for training and testing our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = FreqDist(w.lower() for w in all_words)  # creates Frequency Distribution object with info from all_words\n",
    "\n",
    "word_features = []\n",
    "feature_sets = []\n",
    "\n",
    "for item in all_words.most_common(2000):    # adds the most common 2000 words to word_features\n",
    "    word = item[0]\n",
    "    word_features.append(word)\n",
    "    \n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "    \n",
    "for (review, pos_neg) in documents:     # for each review, what common words does it contain and is it pos or neg?\n",
    "    features = document_features(nltk.word_tokenize(review_list[review]), word_features)\n",
    "    feature_sets.append(tuple((features, pos_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're finally at the point where we can train and test our classifier!\n",
    "\n",
    "First, we need to split our feature set (explained above) and allocate some to our training set, and the rest to our testing set. For simplicity, we've decided to use half of our data for training, and half of our data for testing, but later  you'll get a chance to mess with these proportions yourself!\n",
    "\n",
    "Now, let's train our classifier! With just that one line, we're calling on a built-in Naive Bayes' Classifier (thanks Python <3) to identify the proportion of positive to negative reviews for each of the 2000 most common words. \n",
    "\n",
    "Then, we're calling on a built in function (show most informative features) to give us the 25 words with the most contrasting ratios (the biggest and smallest), meaning that they're the 25 words that have the most consistent connotations of either positive or negative. \n",
    "\n",
    "Finally, we pass in our classifier and our testing set into a built in function that will output how accurate our classifier is as a percent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = feature_sets[500:], feature_sets[:500]\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "print('\\n')\n",
    "classifier.show_most_informative_features(25)\n",
    "\n",
    "print('\\nClassifier accuracy: ' + str(nltk.classify.accuracy(classifier, test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the whole classifier built, let's mess with some stuff ourselves!\n",
    "\n",
    "Let's start by seeing how changing the ratio of training to testing data changes the accuracy of our model. We have it currently set to 50% (0.5) training, and 50% (0.5) testing, with the accuracy above.\n",
    "\n",
    "Below, try raising the testing set size. What do you think will happen to the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid = False\n",
    "while not valid:\n",
    "    test_size = input('Enter a percent (as a decimal) for the testing set size: ')\n",
    "    if float(test_size) > 0 and float(test_size) < 1:\n",
    "        valid = True\n",
    "    else:\n",
    "        print('Please enter a valid input between 0 and 1 (non-inclusive)')\n",
    "test_size = int(float(test_size) * len(documents))\n",
    "\n",
    "test_set, train_set = feature_sets[:test_size], feature_sets[test_size:]\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "print('\\n')\n",
    "classifier.show_most_informative_features(25)\n",
    "\n",
    "print('\\nClassifier accuracy: ' + str(nltk.classify.accuracy(classifier, test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we increase the proportion of our testing set, we're leaving less data to train our classifier. This means it's likely the accuracy of our classifer will decrease because the more data we have to work with in training our model, the more kinds of cases our classifier will encounter, strengthening its accuracy. The most informative features section will also likely look different now, and may make less sense just by glance than when we had a larger training set. \n",
    "\n",
    "Now try to lower our testing set size to below our default 0.5. What do you think will happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid = False\n",
    "while not valid:\n",
    "    test_size = input('Enter a percent (as a decimal) for the testing set size: ')\n",
    "    if float(test_size) > 0 and float(test_size) < 1:\n",
    "        valid = True\n",
    "    else:\n",
    "        print('Please enter a valid input between 0 and 1 (non-inclusive)')\n",
    "test_size = int(float(test_size) * len(documents))\n",
    "\n",
    "test_set, train_set = feature_sets[:test_size], feature_sets[test_size:]\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "print('\\n')\n",
    "classifier.show_most_informative_features(25)\n",
    "\n",
    "print('\\nClassifier accuracy: ' + str(nltk.classify.accuracy(classifier, test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowering our testing set means we have more to work with for training, meaning our accuracy should increase! Just the decision of how much data we're allocating to testing vs training can have signicant effects on how useful our classifier is.\n",
    "\n",
    "\n",
    "Now that we've put in all this work to create and understand this classifer, let's try entering a review ourselves! Go ahead and write up a review for your favorite or least favorite Mexican restaurant! Or whatever you want we can't stop you :~)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = feature_sets[500:], feature_sets[:500]\n",
    "text = input('Enter a review: ')\n",
    "text = nltk.word_tokenize(text)\n",
    "featset = document_features(text, word_features)\n",
    "test_set = [tuple((featset, 'pos'))]\n",
    "result = nltk.classify.accuracy(classifier, test_set)\n",
    "if result == 0:\n",
    "    print('\\nClassified as negative')\n",
    "elif result == 1:\n",
    "    print('\\nClassified as positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations and Pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although our Naive Bayes' Classifier can be useful, there are also some limitations inherent in its design and assumptions. This means that we can mess with our classifier based on what's in the review we're giving it.\n",
    "\n",
    "Let's see this in action! If you need a prompt to get those creative juices flowing, write a review of a restaurant in Eagle Rock!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = feature_sets[500:], feature_sets[:500]\n",
    "text = input('Enter a review with an idiom: ')\n",
    "text = nltk.word_tokenize(text)\n",
    "featset = document_features(text, word_features)\n",
    "test_set = [tuple((featset, 'pos'))]\n",
    "result = nltk.classify.accuracy(classifier, test_set)\n",
    "if result == 0:\n",
    "    print('\\nClassified as negative')\n",
    "elif result == 1:\n",
    "    print('\\nClassified as positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it classify your review correctly? If there's an idiom in a review, there's a good chance our classifier had trouble placing it in the right category. One potential problem is that our classifier looks at each word separately, meaning we can't look at the context around words to determine their connotations the way we do in the \"real world.\" Things like the idiom you entered may lead to misclassifying a review!\n",
    "\n",
    "Let's try another trick! If you need a prompt, write a review of the most expensive meal you've had!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = feature_sets[500:], feature_sets[:500]\n",
    "text = input('Enter a review with double negatives: ')\n",
    "text = nltk.word_tokenize(text)\n",
    "featset = document_features(text, word_features)\n",
    "test_set = [tuple((featset, 'pos'))]\n",
    "result = nltk.classify.accuracy(classifier, test_set)\n",
    "if result == 0:\n",
    "    print('\\nClassified as negative')\n",
    "elif result == 1:\n",
    "    print('\\nClassified as positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did our classifier do with that review? There's a good chance that it was classified as negative, since \"not\" has a pretty strong relation to negative reviews. Even though we can read \"not not\" or \"not bad\" as cancelling out to make a positive, because our classifier looks at each word separately, it's going to just see two separate words with negative connotations. Independence is an assumption we make when we use a Naive Bayes' Classifer, so there's no \"fix\" for this within our classifier as it is.\n",
    "\n",
    "Let's try one more trick! If you need a prompt, review a restaurant you associate with a very specific memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = feature_sets[500:], feature_sets[:500]\n",
    "text = input('Enter a review with rarer words: ')\n",
    "text = nltk.word_tokenize(text)\n",
    "featset = document_features(text, word_features)\n",
    "test_set = [tuple((featset, 'pos'))]\n",
    "result = nltk.classify.accuracy(classifier, test_set)\n",
    "if result == 0:\n",
    "    print('\\nClassified as negative')\n",
    "elif result == 1:\n",
    "    print('\\nClassified as positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did our classifier do now? Because we have less data on rarer words since they show up less, our classification of their negative or positive connotation isn't as accurate. Remember, the more data, the better, but we're always going to run into words that are less and more rare within our set. Because of these rarer words, our classifier may have had a hard time classifying your review.\n",
    "\n",
    "We've now used 3 tricks that mess with the performance of our classifier all because of the assumptions we made when making it. Now, lets look at some takeaways and what we've learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a journey! We started not even knowing what NLP means, and we ended understanding the intricacies of the limitations of a Naive Bayes' Classifier. Kudos to you!\n",
    "\n",
    "This is just the tip of the iceberg when it comes to natural language processing. There are so many different classifiers we could apply to the same area of restaurant reviews with much different results. Further, there are so many more questions we could explore using NLP techniques. We hope you feel equipped enough to "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
